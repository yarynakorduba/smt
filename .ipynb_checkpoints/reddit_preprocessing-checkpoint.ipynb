{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for the Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T23:22:13.981838Z",
     "start_time": "2023-05-31T23:22:13.429790Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from pyarrow import feather\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "\n",
    "# TODO: at first execution download nltk stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T23:22:18.720762Z",
     "start_time": "2023-05-31T23:22:13.984079Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_json('reddit_res/reddit_data.json').set_index('id')\n",
    "\n",
    "# drop linked resources\n",
    "data.drop(columns=['permalink', 'url'], inplace=True)\n",
    "# remove authors as the twitter dataset doesn't have that either\n",
    "# TODO: decide if we want to keep it\n",
    "data.drop(columns=['author'], inplace=True)\n",
    "\n",
    "post_data = data.drop(columns=['comments']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def process_comments(comment_df_list_, comment_data_):\n",
    "    for comments_ in comment_data_:\n",
    "        if len(comments_) == 0:\n",
    "            continue\n",
    "        comments_ = pd.DataFrame.from_records(comments_).set_index('id')\n",
    "        comments_['created'] = comments_['created'].astype(int)\n",
    "        comments_.drop(columns=['permalink', 'author'], inplace=True)\n",
    "        comments_.reset_index(drop=True, inplace=True)\n",
    "        process_comments(comment_df_list_, comments_['comments']) # process comments of comments\n",
    "        comment_df_list_.append(comments_.drop(columns=['comments']))\n",
    "\n",
    "\n",
    "comment_list_dfs = []\n",
    "process_comments(comment_list_dfs, data['comments'])\n",
    "comment_data = pd.concat(comment_list_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete English Comments and Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_english_text(text):\n",
    "    if len(text) >= 3: \n",
    "        try:\n",
    "            if detect(text) != 'en':\n",
    "                return text\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data[\"text\"] = post_data[\"text\"].apply(remove_english_text)\n",
    "comment_data[\"text\"] = comment_data[\"text\"].apply(remove_english_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_data.dropna(inplace=True)\n",
    "comment_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T23:22:20.579449Z",
     "start_time": "2023-05-31T23:22:18.724327Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', r\"\", text)\n",
    "\n",
    "post_data['title'] = post_data['title'].apply(lambda title: remove_url(title))\n",
    "post_data['text'] = post_data['text'].apply(lambda text: remove_url(text))\n",
    "comment_data['text'] = comment_data['text'].apply(lambda text: remove_url(text))\n",
    "\n",
    "# remove words with length less than or equal to 2 and remove words that contain only digits\n",
    "post_data['text'] = post_data['text'].apply(lambda text: ' '.join(word for word in text.split() if len(word) > 2 and not re.match(r'^\\d+$', word)))\n",
    "#post_data['text'] = post_data['text'].apply(lambda text: ' '.join(word for word in text.split() if len(word) >= 2 and not word.isdigit()))\n",
    "comment_data['text'] = comment_data['text'].apply(lambda text: ' '.join(word for word in text.split() if len(word) >= 2 and not word.isdigit()))\n",
    "\n",
    "#\n",
    "post_data['text'] = post_data['text'].astype(str).fillna(\"\").str.replace(r'[<>\\!\\[\\]\\(\\)\\{\\}\\^\\*\\+\\-\\=\\/\\\\\\|#&]+.', '', regex=True)\n",
    "comment_data['text'] = comment_data['text'].astype(str).fillna(\"\").str.replace(r'[<>\\!\\[\\]\\(\\)\\{\\}\\^\\*\\+\\-\\=\\/\\\\\\|#&]+.', '', regex=True)\n",
    "\n",
    "\n",
    "# remove just the apostrophe and any characters following it\n",
    "post_data['text'] = post_data['text'].apply(lambda x: re.sub(r\"'[^ ]*\", '', x))\n",
    "comment_data['text'] = comment_data['text'].apply(lambda x: re.sub(r\"'[^ ]*\", '', x))\n",
    "\n",
    "\n",
    "\n",
    "def general_cleaning(stop_words_, text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language='german')\n",
    "    text_tmp = \"\"\n",
    "\n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if token_low not in stop_words_:\n",
    "            text_tmp += token_low + ' '\n",
    "\n",
    "    if text_tmp == \"\":\n",
    "        return pd.NA\n",
    "    return text_tmp\n",
    "\n",
    " \n",
    "with open(\"stopwords-de.txt\", \"r\") as file:\n",
    "    additional_stop_words = set(file.read().splitlines())\n",
    "\n",
    "        \n",
    "stop_words = set(additional_stop_words)\n",
    "#stop_words.update(additional_stop_words)\n",
    "#stop_words.add(s)\n",
    "stop_words.add('\\n')\n",
    "stop_words.add('\\\\-')\n",
    "stop_words.add('http')\n",
    "stop_words.add('https')\n",
    "stop_words.add('//')\n",
    "stop_words.add('(')\n",
    "stop_words.add(')')\n",
    "stop_words.add('[')\n",
    "stop_words.add(']')\n",
    "stop_words.add('„')\n",
    "stop_words.add('#')\n",
    "\n",
    "\n",
    "\n",
    "# TODO: decide if we want to remove , ; : and so on\n",
    "stop_words.add('.')\n",
    "stop_words.add('*')\n",
    "stop_words.add(\"\\\\\")\n",
    "stop_words.add('“')\n",
    "stop_words.add('%')\n",
    "stop_words.add('\\'')\n",
    "stop_words.add('+')\n",
    "stop_words.add('ja')\n",
    "stop_words.add('is')\n",
    "stop_words.add('by')\n",
    "stop_words.add('a')\n",
    "stop_words.add('i')\n",
    "stop_words.add('”')\n",
    "stop_words.add('!')\n",
    "stop_words.add('?')\n",
    "stop_words.add(',')\n",
    "stop_words.add(';')\n",
    "stop_words.add(':')\n",
    "stop_words.add('|')\n",
    "stop_words.add('<')\n",
    "stop_words.add('>')\n",
    "stop_words.add('-')\n",
    "stop_words.add('..')\n",
    "stop_words.add('...')\n",
    "stop_words.add(\"''\")\n",
    "stop_words.add('``')\n",
    "stop_words.add('´´')\n",
    "# remove more if needed!\n",
    "\n",
    "\n",
    "post_data['subreddit'] = post_data['subreddit'].str.lower()\n",
    "post_data['title'] = post_data['title'].apply(lambda title: general_cleaning(stop_words, title))\n",
    "post_data['text'] = post_data['text'].apply(lambda text: general_cleaning(stop_words, text))\n",
    "comment_data['text'] = comment_data['text'].apply(lambda text: general_cleaning(stop_words, text))\n",
    "\n",
    "post_data.dropna(inplace=True)\n",
    "comment_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get timestamps in human-readable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T23:22:20.590586Z",
     "start_time": "2023-05-31T23:22:20.580450Z"
    }
   },
   "outputs": [],
   "source": [
    "post_data['created'] = post_data['created'].apply(lambda timestamp: datetime.utcfromtimestamp(timestamp))\n",
    "comment_data['created'] = comment_data['created'].apply(lambda timestamp: datetime.utcfromtimestamp(timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export as Feather Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T23:22:20.608801Z",
     "start_time": "2023-05-31T23:22:20.589564Z"
    }
   },
   "outputs": [],
   "source": [
    "feather.write_feather(post_data, 'reddit_res/posts.ftr')\n",
    "feather.write_feather(comment_data, 'reddit_res/comments.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
