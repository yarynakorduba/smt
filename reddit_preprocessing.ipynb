{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing for the Reddit Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import nltk\n",
    "from pyarrow import feather\n",
    "from datetime import datetime\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# TODO: at first execution download nltk stopwords\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T23:22:13.429790Z",
     "end_time": "2023-05-31T23:22:13.981838Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Loading"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_json('reddit_res/reddit_data.json').set_index('id')\n",
    "\n",
    "# drop linked resources\n",
    "data.drop(columns=['permalink', 'url'], inplace=True)\n",
    "# remove authors as the twitter dataset doesn't have that either\n",
    "# TODO: decide if we want to keep it\n",
    "data.drop(columns=['author'], inplace=True)\n",
    "\n",
    "post_data = data.drop(columns=['comments']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def process_comments(comment_df_list_, comment_data_):\n",
    "    for comments_ in comment_data_:\n",
    "        if len(comments_) == 0:\n",
    "            continue\n",
    "        comments_ = pd.DataFrame.from_records(comments_).set_index('id')\n",
    "        comments_['created'] = comments_['created'].astype(int)\n",
    "        comments_.drop(columns=['permalink', 'author'], inplace=True)\n",
    "        comments_.reset_index(drop=True, inplace=True)\n",
    "        process_comments(comment_df_list_, comments_['comments']) # process comments of comments\n",
    "        comment_df_list_.append(comments_.drop(columns=['comments']))\n",
    "\n",
    "\n",
    "comment_list_dfs = []\n",
    "process_comments(comment_list_dfs, data['comments'])\n",
    "comment_data = pd.concat(comment_list_dfs).reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T23:22:13.984079Z",
     "end_time": "2023-05-31T23:22:18.720762Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Clean Data\n",
    "- based on twitter preprocessing\n",
    "- also removes links"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', r\"\", text)\n",
    "\n",
    "post_data['title'] = post_data['title'].apply(lambda title: remove_url(title))\n",
    "post_data['text'] = post_data['text'].apply(lambda text: remove_url(text))\n",
    "comment_data['text'] = comment_data['text'].apply(lambda text: remove_url(text))\n",
    "\n",
    "\n",
    "def general_cleaning(stop_words_, text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text, language='german')\n",
    "    text_tmp = \"\"\n",
    "\n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if token_low not in stop_words_:\n",
    "            text_tmp += token_low + ' '\n",
    "\n",
    "    if text_tmp == \"\":\n",
    "        return pd.NA\n",
    "    return text_tmp\n",
    "\n",
    "stop_words = set(stopwords.words('german'))\n",
    "stop_words.add('\\n')\n",
    "stop_words.add('\\\\-')\n",
    "stop_words.add('http')\n",
    "stop_words.add('https')\n",
    "stop_words.add('//')\n",
    "stop_words.add('(')\n",
    "stop_words.add(')')\n",
    "stop_words.add('[')\n",
    "stop_words.add(']')\n",
    "stop_words.add('removed')\n",
    "stop_words.add('deleted')\n",
    "\n",
    "# TODO: decide if we want to remove , ; : and so on\n",
    "stop_words.add('!')\n",
    "stop_words.add('.')\n",
    "stop_words.add('?')\n",
    "stop_words.add(',')\n",
    "stop_words.add(';')\n",
    "stop_words.add(':')\n",
    "stop_words.add('|')\n",
    "stop_words.add('<')\n",
    "stop_words.add('>')\n",
    "stop_words.add('-')\n",
    "stop_words.add('..')\n",
    "stop_words.add('...')\n",
    "stop_words.add(\"''\")\n",
    "stop_words.add('``')\n",
    "stop_words.add('´´')\n",
    "# remove more if needed!\n",
    "\n",
    "\n",
    "post_data['subreddit'] = post_data['subreddit'].str.lower()\n",
    "post_data['title'] = post_data['title'].apply(lambda title: general_cleaning(stop_words, title))\n",
    "post_data['text'] = post_data['text'].apply(lambda text: general_cleaning(stop_words, text))\n",
    "\n",
    "comment_data['text'] = comment_data['text'].apply(lambda text: general_cleaning(stop_words, text))\n",
    "\n",
    "post_data.dropna(inplace=True)\n",
    "comment_data.dropna(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T23:22:18.724327Z",
     "end_time": "2023-05-31T23:22:20.579449Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get timestamps in human-readable format"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "post_data['created'] = post_data['created'].apply(lambda timestamp: datetime.utcfromtimestamp(timestamp))\n",
    "comment_data['created'] = comment_data['created'].apply(lambda timestamp: datetime.utcfromtimestamp(timestamp))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T23:22:20.580450Z",
     "end_time": "2023-05-31T23:22:20.590586Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export as Feather Datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "feather.write_feather(post_data, 'reddit_res/posts.ftr')\n",
    "feather.write_feather(comment_data, 'reddit_res/comments.ftr')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-05-31T23:22:20.589564Z",
     "end_time": "2023-05-31T23:22:20.608801Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
