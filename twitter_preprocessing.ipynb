{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32b46cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:57:54.709748300Z",
     "start_time": "2023-05-20T17:57:53.590674800Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pyarrow\n",
    "import regex as re\n",
    "import langid\n",
    "from datetime import datetime\n",
    "from LanguageDetection import LanguageDetection as lang\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc12c5ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:04.677478400Z",
     "start_time": "2023-05-20T17:57:56.931770100Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data and drop obsolete columns\n",
    "\n",
    "df_twitter_old = pd.read_csv('twitter_res/twitter_sentiment_data.csv')\n",
    "df_twitter_old.drop(columns=['sentiment'], inplace=True)\n",
    "df_twitter_old.rename(columns={'message':'tweet', 'tweetid':'tweetsid'}, inplace=True)\n",
    "\n",
    "df_twitter_new = pd.read_csv('twitter_res/climate_change_tweets.csv')\n",
    "df_twitter_new.drop(columns=['hashtag'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e515a11e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:04.788581200Z",
     "start_time": "2023-05-20T17:58:04.680482700Z"
    }
   },
   "outputs": [],
   "source": [
    "# concat\n",
    "\n",
    "df_twitter = pd.concat([df_twitter_old, df_twitter_new])\n",
    "df_twitter = df_twitter.reset_index()\n",
    "df_twitter = df_twitter.head(500)\n",
    "\n",
    "#df_twitter.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Clean Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**General cleaning (remove stopwords and transform to lowercase)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def general_cleaning(tweet):\n",
    "    tokens = tk.tokenize(tweet.encode('ascii', errors='ignore').decode())\n",
    "    tweet_tmp = \"\"\n",
    "\n",
    "    for token in tokens:\n",
    "        token_low = token.lower()\n",
    "        if token_low not in stop_words:\n",
    "            tweet_tmp += token_low + ' '\n",
    "\n",
    "    return tweet_tmp\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('rt')\n",
    "stop_words.add('htt')\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "df_twitter[\"tweet\"] = df_twitter[\"tweet\"].apply((lambda tweet: general_cleaning(tweet)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:04.828617Z",
     "start_time": "2023-05-20T17:58:04.789582200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "57ea071d",
   "metadata": {},
   "source": [
    "**Remove URLs and users marked with @**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1becf2ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:05.794494200Z",
     "start_time": "2023-05-20T17:58:05.783484700Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_url(tweet):\n",
    "    return re.sub(r'https?://\\S+|www\\.\\S+', r\"\", tweet)\n",
    "\n",
    "\n",
    "def remove_user(tweet):\n",
    "    return re.sub(r'@[^\\s]+', r\"\", tweet)\n",
    "\n",
    "# Remove @ and urls\n",
    "df_twitter[\"tweet\"] = df_twitter[\"tweet\"].apply((lambda tweet: remove_url(tweet)))\n",
    "df_twitter[\"tweet\"] = df_twitter[\"tweet\"].apply((lambda tweet: remove_user(tweet)))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Drop Duplicates**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "df_twitter.drop_duplicates(subset='tweet', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:08.674160200Z",
     "start_time": "2023-05-20T17:58:08.663645600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "eddd45eb",
   "metadata": {},
   "source": [
    "**Drop rows, where the tweet is not in English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d771bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:12.508160500Z",
     "start_time": "2023-05-20T17:58:10.180062700Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_language(tweet):\n",
    "   # language = lang.LanguageDetection().LanguageDetect(tweet)\n",
    "   # language = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "    pred = langid.classify(tweet)\n",
    "    if pred[0] != \"en\":\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# Filtering language\n",
    "to_drop = []\n",
    "df_twitter['tweet'] = df_twitter['tweet'].apply(lambda tweet: tweet if (detect_language(tweet)) else to_drop.append(\n",
    "    df_twitter[df_twitter[\"tweet\"] == tweet].index[0]))\n",
    "df_twitter = df_twitter.drop(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Other"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "id": "728d8337",
   "metadata": {},
   "source": [
    "**Add column with date encoding the ID from the tweets**\n",
    "Source: https://github.com/oduwsdl/tweetedat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b04c94ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:16.000338600Z",
     "start_time": "2023-05-20T17:58:15.992333900Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_timestamp(id):\n",
    "    time = datetime.utcfromtimestamp(((id >> 22) + 1288834974657) / 1000)\n",
    "    return time.strftime(\"%d-%m-%Y\")\n",
    "\n",
    "df_twitter['tweet_ts'] = df_twitter['tweetsid'].apply(lambda id: get_timestamp(id))\n",
    "df_twitter = df_twitter.drop(columns=\"tweetsid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92576129",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:17.851023700Z",
     "start_time": "2023-05-20T17:58:17.839012100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     index                                              tweet    tweet_ts\n",
      "0        0   climate change interesting hustle global warm...  31-10-2016\n",
      "1        1   watch #beforetheflood right here,  travels wo...  31-10-2016\n",
      "2        2  fabulous! leonardo #dicaprio's film #climate c...  31-10-2016\n",
      "3        3   watched amazing documentary leonardodicaprio ...  31-10-2016\n",
      "4        4   pranita biswasi, lutheran odisha, gives testi...  31-10-2016\n",
      "..     ...                                                ...         ...\n",
      "494    494  cartoon: media covered climate change way cove...  01-11-2016\n",
      "496    496   effect climate change food chain huge  #anima...  01-11-2016\n",
      "497    497   youre enjoying warm weather lowkey know globa...  01-11-2016\n",
      "498    498   appetite oil gas continue grow despite effort...  01-11-2016\n",
      "499    499   via  cartoon: media covered climate change wa...  01-11-2016\n",
      "\n",
      "[281 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Add column with hashtags**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    return re.findall('#\\w+', tweet)\n",
    "\n",
    "df_twitter['hashtags']= df_twitter['tweet'].apply(lambda tweet: get_hashtags(tweet))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-20T17:58:21.425260500Z",
     "start_time": "2023-05-20T17:58:21.416252Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Write twitter dataframe to binary Feather format**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def write_to_feather():\n",
    "    root_dir = os.path.dirname(os.path.abspath('twitter_preprocessing.ipynb'))\n",
    "    path = os.path.join(root_dir, 'twitter_res/twitter.ftr')\n",
    "    pyarrow.feather.write_feather(df_twitter, path)\n",
    "\n",
    "# read: df_twitter = pd.read_feather(path);"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
